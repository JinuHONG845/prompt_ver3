import streamlit as st
from openai import OpenAI
from anthropic import Anthropic
import google.generativeai as genai
import requests

# API í‚¤ ì„¤ì • with ì˜ˆì™¸ ì²˜ë¦¬
try:
    # API í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if "api_keys" not in st.secrets:
        st.error("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì—ì„œ api_keys ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    # ê° API í‚¤ ì„¤ì •
    openai_client = OpenAI(api_key=st.secrets["api_keys"]["OPENAI_API_KEY"])
    anthropic = Anthropic(api_key=st.secrets["api_keys"]["ANTHROPIC_API_KEY"])
    genai.configure(api_key=st.secrets["api_keys"]["GEMINI_API_KEY"])
    PERPLEXITY_API_KEY = st.secrets["api_keys"]["PERPLEXITY_API_KEY"]
    
except Exception as e:
    st.error(f"API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    st.write("í˜„ì¬ ì„¤ì •ëœ secrets ë‚´ìš©:")
    st.write(st.secrets)
    st.stop()

def get_perplexity_response(prompt):
    url = "https://api.perplexity.ai/chat/completions"
    headers = {
        "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "mistral-7b-instruct",
        "messages": [
            {
                "role": "system",
                "content": "Be precise and concise."
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        "max_tokens": 1024,
        "temperature": 0.2,
        "top_p": 0.9,
        "stream": False
    }
    
    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        response_json = response.json()
        
        if 'error' in response_json:
            return f"Perplexity API Error: {response_json['error']}"
            
        if 'choices' in response_json and len(response_json['choices']) > 0:
            return response_json['choices'][0]['message']['content']
        else:
            return "No valid response from Perplexity API"
            
    except requests.exceptions.RequestException as e:
        return f"API Request Error: {str(e)}\nResponse: {response.text if 'response' in locals() else 'No response'}"
    except Exception as e:
        return f"Error processing response: {str(e)}"

def get_chatgpt_response(prompt):
    try:
        client = openai.OpenAI(api_key=st.secrets["api_keys"]["OPENAI_API_KEY"])
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"ChatGPT Error: {str(e)}"

def get_claude_response(prompt):
    message = anthropic.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=1000,
        messages=[{"role": "user", "content": prompt}]
    )
    return message.content[0].text

def get_gemini_response(prompt):
    model = genai.GenerativeModel('gemini-pro')
    response = model.generate_content(prompt)
    return response.text

def main():
    st.title("ğŸ¤– LLM ë¹„êµ ì• í”Œë¦¬ì¼€ì´ì…˜")
    
    # ì‚¬ì´ë“œë°”ì— ëª¨ë¸ ì„ íƒ
    models = st.sidebar.multiselect(
        "ë¹„êµí•  ëª¨ë¸ ì„ íƒ",
        ["Perplexity", "ChatGPT", "Claude", "Gemini"],
        default=["Perplexity", "ChatGPT", "Claude", "Gemini"]
    )
    
    # í”„ë¡¬í”„íŠ¸ ì…ë ¥
    prompt = st.text_area("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", height=100)
    
    if st.button("ì‘ë‹µ ìƒì„±"):
        if not prompt:
            st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return
        
        # Create columns based on selected models
        cols = st.columns(len(models))
        
        # ì„ íƒëœ ëª¨ë¸ë³„ë¡œ ì‘ë‹µ ìƒì„±
        for col, model in zip(cols, models):
            with col:
                st.markdown(f"### {model}")
                with st.spinner(f"{model} ì‘ë‹µ ìƒì„± ì¤‘..."):
                    try:
                        if model == "Perplexity":
                            response = get_perplexity_response(prompt)
                        elif model == "ChatGPT":
                            response = get_chatgpt_response(prompt)
                        elif model == "Claude":
                            response = get_claude_response(prompt)
                        else:  # Gemini
                            response = get_gemini_response(prompt)
                        
                        st.write(response)
                    except Exception as e:
                        st.error(f"{model} ì—ëŸ¬ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    main()
